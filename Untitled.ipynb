{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d93ca158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e3dad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3564b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614ed260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://112.171.115.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x29d338ec580>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6482917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('./num_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80c51a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark # 타입 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "247766ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+\n",
      "|_c0|_c1|_c2|_c3|_c4|_c5|_c6|\n",
      "+---+---+---+---+---+---+---+\n",
      "|  0|  1|  2|  3|  4|  5|  6|\n",
      "|  0|  5|  6| 11| 29| 42| 45|\n",
      "|  1| 12| 15| 17| 24| 29| 45|\n",
      "|  2| 12| 27| 29| 38| 41| 45|\n",
      "|  3|  1|  4| 13| 17| 34| 39|\n",
      "|  4|  3| 19| 21| 25| 37| 45|\n",
      "|  5| 12| 18| 22| 23| 30| 34|\n",
      "|  6| 15| 26| 28| 34| 41| 42|\n",
      "|  7| 14| 23| 31| 33| 37| 40|\n",
      "|  8|  3| 11| 14| 18| 26| 27|\n",
      "|  9| 21| 22| 26| 34| 36| 41|\n",
      "| 10|  5| 11| 18| 20| 35| 45|\n",
      "| 11|  1|  9| 12| 26| 35| 38|\n",
      "| 12|  9| 12| 15| 25| 34| 36|\n",
      "| 13| 15| 23| 29| 34| 40| 44|\n",
      "| 14|  9| 11| 30| 31| 41| 44|\n",
      "| 15|  8| 11| 16| 19| 21| 25|\n",
      "| 16|  8| 11| 15| 16| 17| 37|\n",
      "| 17|  8| 13| 18| 24| 27| 29|\n",
      "| 18|  7| 15| 30| 37| 39| 44|\n",
      "+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show() # 탑20을 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76d76f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[0: string, 1: string, 2: string, 3: string, 4: string, 5: string, 6: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.option('header', True).csv('./num_.csv') # __바로는 에러가 뜸\n",
    "#\n",
    "# 아마도 컬럼명은 int면 안되나? -> 상관없음. 'true' -> True 로 변경 해주면 됨.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef3e78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존의 컬럼명을 str로 변경\n",
    "temp_sample = pd.read_csv('./num_.csv')\n",
    "temp_sample.columns = ['c_'+_col for _col in temp_sample.columns]\n",
    "temp_sample.to_csv('./num__.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a32beb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[c_0: string, c_1: string, c_2: string, c_3: string, c_4: string, c_5: string, c_6: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.option('header', True).csv('./num__.csv') # 버젼이 바뀌면서 str(t/f) 기 bool로 바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3714f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header', True).csv('./num_.csv') # 이렇게하면 int가 str로 전환됨\n",
    "df_pyspark = spark.read.option('header', True).csv('./num_.csv', inferSchema=True) # int는 intㄹ 전화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb1ea715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1c9b634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(0=0, 1=5, 2=6, 3=11, 4=29, 5=42, 6=45),\n",
       " Row(0=1, 1=12, 2=15, 3=17, 4=24, 5=29, 6=45),\n",
       " Row(0=2, 1=12, 2=27, 3=29, 4=38, 5=41, 6=45)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3deb7adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0: integer (nullable = true)\n",
      " |-- 1: integer (nullable = true)\n",
      " |-- 2: integer (nullable = true)\n",
      " |-- 3: integer (nullable = true)\n",
      " |-- 4: integer (nullable = true)\n",
      " |-- 5: integer (nullable = true)\n",
      " |-- 6: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d26aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('./num_.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b55ae386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d101cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[0: int]\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# 단일 컬럼 선택\n",
    "print(df_pyspark.select('0'))\n",
    "print(type(df_pyspark.select('0')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6d5d3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[0: int, 2: int]\n",
      "+---+---+\n",
      "|  0|  2|\n",
      "+---+---+\n",
      "|  0|  6|\n",
      "|  1| 15|\n",
      "|  2| 27|\n",
      "|  3|  4|\n",
      "|  4| 19|\n",
      "|  5| 18|\n",
      "|  6| 26|\n",
      "|  7| 23|\n",
      "|  8| 11|\n",
      "|  9| 22|\n",
      "| 10| 11|\n",
      "| 11|  9|\n",
      "| 12| 12|\n",
      "| 13| 23|\n",
      "| 14| 11|\n",
      "| 15| 11|\n",
      "| 16| 11|\n",
      "| 17| 13|\n",
      "| 18| 15|\n",
      "| 19|  4|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 다중 컬럼 선택\n",
    "print(df_pyspark.select(['0', '2']))\n",
    "df_pyspark.select(['0', '2']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14669a81",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_pyspark\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "# select를 해야함 안하면 그냥 오류\n",
    "df_pyspark['0'].show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e41b3b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 'int'),\n",
       " ('1', 'int'),\n",
       " ('2', 'int'),\n",
       " ('3', 'int'),\n",
       " ('4', 'int'),\n",
       " ('5', 'int'),\n",
       " ('6', 'int')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ba016bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------------------+-----------------+------------------+-----------------+-----------------+\n",
      "|summary|                0|                1|                 2|                3|                 4|                5|                6|\n",
      "+-------+-----------------+-----------------+------------------+-----------------+------------------+-----------------+-----------------+\n",
      "|  count|             1022|             1022|              1022|             1022|              1022|             1022|             1022|\n",
      "|   mean|            510.5|6.658512720156556|13.051859099804306|19.94618395303327|26.265166340508806|32.86497064579256|39.52739726027397|\n",
      "| stddev|295.1702898328353|5.324331168246601| 6.867020894994583|7.457489062148432| 7.606323721100002|6.879212960723106|5.292125697149428|\n",
      "|    min|                0|                1|                 2|                3|                 5|                9|               19|\n",
      "|    max|             1021|               35|                37|               40|                43|               44|               45|\n",
      "+-------+-----------------+-----------------+------------------+-----------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c02925bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+-----+\n",
      "|  0|  1|  2|  3|  4|  5|  6|bonus|\n",
      "+---+---+---+---+---+---+---+-----+\n",
      "|  0|  5|  6| 11| 29| 42| 45|   13|\n",
      "|  1| 12| 15| 17| 24| 29| 45|   19|\n",
      "|  2| 12| 27| 29| 38| 41| 45|   31|\n",
      "|  3|  1|  4| 13| 17| 34| 39|   15|\n",
      "|  4|  3| 19| 21| 25| 37| 45|   23|\n",
      "|  5| 12| 18| 22| 23| 30| 34|   24|\n",
      "|  6| 15| 26| 28| 34| 41| 42|   30|\n",
      "|  7| 14| 23| 31| 33| 37| 40|   33|\n",
      "|  8|  3| 11| 14| 18| 26| 27|   16|\n",
      "|  9| 21| 22| 26| 34| 36| 41|   28|\n",
      "| 10|  5| 11| 18| 20| 35| 45|   20|\n",
      "| 11|  1|  9| 12| 26| 35| 38|   14|\n",
      "| 12|  9| 12| 15| 25| 34| 36|   17|\n",
      "| 13| 15| 23| 29| 34| 40| 44|   31|\n",
      "| 14|  9| 11| 30| 31| 41| 44|   32|\n",
      "| 15|  8| 11| 16| 19| 21| 25|   18|\n",
      "| 16|  8| 11| 15| 16| 17| 37|   17|\n",
      "| 17|  8| 13| 18| 24| 27| 29|   20|\n",
      "| 18|  7| 15| 30| 37| 39| 44|   32|\n",
      "| 19|  1|  4| 29| 39| 43| 45|   31|\n",
      "+---+---+---+---+---+---+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add col\n",
    "\n",
    "df_pyspark = df_pyspark.withColumn('bonus', df_pyspark['3']+2)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f5275e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+\n",
      "|  0|  1|  2|  3|  4|  5|  6|\n",
      "+---+---+---+---+---+---+---+\n",
      "|  0|  5|  6| 11| 29| 42| 45|\n",
      "|  1| 12| 15| 17| 24| 29| 45|\n",
      "|  2| 12| 27| 29| 38| 41| 45|\n",
      "|  3|  1|  4| 13| 17| 34| 39|\n",
      "|  4|  3| 19| 21| 25| 37| 45|\n",
      "|  5| 12| 18| 22| 23| 30| 34|\n",
      "|  6| 15| 26| 28| 34| 41| 42|\n",
      "|  7| 14| 23| 31| 33| 37| 40|\n",
      "|  8|  3| 11| 14| 18| 26| 27|\n",
      "|  9| 21| 22| 26| 34| 36| 41|\n",
      "| 10|  5| 11| 18| 20| 35| 45|\n",
      "| 11|  1|  9| 12| 26| 35| 38|\n",
      "| 12|  9| 12| 15| 25| 34| 36|\n",
      "| 13| 15| 23| 29| 34| 40| 44|\n",
      "| 14|  9| 11| 30| 31| 41| 44|\n",
      "| 15|  8| 11| 16| 19| 21| 25|\n",
      "| 16|  8| 11| 15| 16| 17| 37|\n",
      "| 17|  8| 13| 18| 24| 27| 29|\n",
      "| 18|  7| 15| 30| 37| 39| 44|\n",
      "| 19|  1|  4| 29| 39| 43| 45|\n",
      "+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop col\n",
    "df_pyspark = df_pyspark.drop('bonus')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e5e3e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename col\n",
    "df_pyspark = df_pyspark.withColumnRenamed('0', 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2a79c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rename col multi\n",
    "new_names = ['index'] + [f'col_{i}' for i in range(1, 7)]\n",
    "df_pyspark = df_pyspark.toDF(*new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e900abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|index|col_1|col_2|col_3|col_4|col_5|col_6|\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|    0|    5|    6|   11|   29|   42|   45|\n",
      "|    1|   12|   15|   17|   24|   29|   45|\n",
      "|    2|   12|   27|   29|   38|   41|   45|\n",
      "|    3|    1|    4|   13|   17|   34|   39|\n",
      "|    4|    3|   19|   21|   25|   37|   45|\n",
      "|    5|   12|   18|   22|   23|   30|   34|\n",
      "|    6|   15|   26|   28|   34|   41|   42|\n",
      "|    7|   14|   23|   31|   33|   37|   40|\n",
      "|    8|    3|   11|   14|   18|   26|   27|\n",
      "|    9|   21|   22|   26|   34|   36|   41|\n",
      "|   10|    5|   11|   18|   20|   35|   45|\n",
      "|   11|    1|    9|   12|   26|   35|   38|\n",
      "|   12|    9|   12|   15|   25|   34|   36|\n",
      "|   13|   15|   23|   29|   34|   40|   44|\n",
      "|   14|    9|   11|   30|   31|   41|   44|\n",
      "|   15|    8|   11|   16|   19|   21|   25|\n",
      "|   16|    8|   11|   15|   16|   17|   37|\n",
      "|   17|    8|   13|   18|   24|   27|   29|\n",
      "|   18|    7|   15|   30|   37|   39|   44|\n",
      "|   19|    1|    4|   29|   39|   43|   45|\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f0153434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_pyspark.na.drop() == df_dropna() :: null 들어간 행 다 날림\\ndf_pyspark.na.drop(how='any') : default\\ndf_pyspark.na.drop(how='all') : 이 경우는 모든 열에서 null 이어야 삭제됨\\ndf_pyspark.na.drop(how='any', thresh=2) : 각 행의 (열 - null)값이 thresh값을 초과인 행은 삭제 \\ndf_pyspark.na.drop(how='any', subset=[col_name]) : col_name에 null값이 있는 행 삭제\\n\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## drop\n",
    "\"\"\"\n",
    "df_pyspark.na.drop() == df_dropna() :: null 들어간 행 다 날림\n",
    "df_pyspark.na.drop(how='any') : default\n",
    "df_pyspark.na.drop(how='all') : 이 경우는 모든 열에서 null 이어야 삭제됨\n",
    "df_pyspark.na.drop(how='any', thresh=2) : 각 행의 (열 - null)값이 thresh값을 초과인 행은 삭제 \n",
    "df_pyspark.na.drop(how='any', subset=[col_name]) : col_name에 null값이 있는 행 삭제\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15139b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill\n",
    "\"\"\"\n",
    "df_pyspark.na.fill('Missing Values') : null을 Missing Values로 채운다.\n",
    "df_pyspark.na.fill('Missing Values', [col_name]) : col_name의 null을 Missing Values로 채운다. col_name이 아닌 열의 null은 그대로\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867f6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73a1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols=[str(i) for i in range(1, 7)],\n",
    "    outputCols=[f\"{i}_imputed\" for i in range(1, 7)]\n",
    ").setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f5df873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+---+---+---+---+---------+---------+---------+---------+---------+---------+\n",
      "|index|  1|  2|  3|  4|  5|  6|1_imputed|2_imputed|3_imputed|4_imputed|5_imputed|6_imputed|\n",
      "+-----+---+---+---+---+---+---+---------+---------+---------+---------+---------+---------+\n",
      "|    0|  5|  6| 11| 29| 42| 45|        5|        6|       11|       29|       42|       45|\n",
      "|    1| 12| 15| 17| 24| 29| 45|       12|       15|       17|       24|       29|       45|\n",
      "|    2| 12| 27| 29| 38| 41| 45|       12|       27|       29|       38|       41|       45|\n",
      "|    3|  1|  4| 13| 17| 34| 39|        1|        4|       13|       17|       34|       39|\n",
      "|    4|  3| 19| 21| 25| 37| 45|        3|       19|       21|       25|       37|       45|\n",
      "|    5| 12| 18| 22| 23| 30| 34|       12|       18|       22|       23|       30|       34|\n",
      "|    6| 15| 26| 28| 34| 41| 42|       15|       26|       28|       34|       41|       42|\n",
      "|    7| 14| 23| 31| 33| 37| 40|       14|       23|       31|       33|       37|       40|\n",
      "|    8|  3| 11| 14| 18| 26| 27|        3|       11|       14|       18|       26|       27|\n",
      "|    9| 21| 22| 26| 34| 36| 41|       21|       22|       26|       34|       36|       41|\n",
      "|   10|  5| 11| 18| 20| 35| 45|        5|       11|       18|       20|       35|       45|\n",
      "|   11|  1|  9| 12| 26| 35| 38|        1|        9|       12|       26|       35|       38|\n",
      "|   12|  9| 12| 15| 25| 34| 36|        9|       12|       15|       25|       34|       36|\n",
      "|   13| 15| 23| 29| 34| 40| 44|       15|       23|       29|       34|       40|       44|\n",
      "|   14|  9| 11| 30| 31| 41| 44|        9|       11|       30|       31|       41|       44|\n",
      "|   15|  8| 11| 16| 19| 21| 25|        8|       11|       16|       19|       21|       25|\n",
      "|   16|  8| 11| 15| 16| 17| 37|        8|       11|       15|       16|       17|       37|\n",
      "|   17|  8| 13| 18| 24| 27| 29|        8|       13|       18|       24|       27|       29|\n",
      "|   18|  7| 15| 30| 37| 39| 44|        7|       15|       30|       37|       39|       44|\n",
      "|   19|  1|  4| 29| 39| 43| 45|        1|        4|       29|       39|       43|       45|\n",
      "+-----+---+---+---+---+---+---+---------+---------+---------+---------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()\n",
    "# 이건 null 값을 평균값으로 채워준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b753cbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+\n",
      "|  0|  1|  2|  3|  4|  5|  6|\n",
      "+---+---+---+---+---+---+---+\n",
      "|  0|  5|  6| 11| 29| 42| 45|\n",
      "|  1| 12| 15| 17| 24| 29| 45|\n",
      "|  2| 12| 27| 29| 38| 41| 45|\n",
      "|  3|  1|  4| 13| 17| 34| 39|\n",
      "|  4|  3| 19| 21| 25| 37| 45|\n",
      "+---+---+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5754112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|index|col_1|col_2|col_3|col_4|col_5|col_6|\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|    2|   12|   27|   29|   38|   41|   45|\n",
      "|    4|    3|   19|   21|   25|   37|   45|\n",
      "|    5|   12|   18|   22|   23|   30|   34|\n",
      "|    6|   15|   26|   28|   34|   41|   42|\n",
      "|    7|   14|   23|   31|   33|   37|   40|\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|index|col_1|col_2|col_3|col_4|col_5|col_6|\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|    0|    5|    6|   11|   29|   42|   45|\n",
      "|    1|   12|   15|   17|   24|   29|   45|\n",
      "|    3|    1|    4|   13|   17|   34|   39|\n",
      "|    8|    3|   11|   14|   18|   26|   27|\n",
      "|   10|    5|   11|   18|   20|   35|   45|\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas 하고 유사함\n",
    "df_pyspark.filter('col_3 >= 20').show(5) # str 상태의 조건문은 ~가 안됨\n",
    "df_pyspark.filter(~(df_pyspark['col_3'] >= 20)).show(5) # 이렇게는 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "132a0bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+----------+----------+----------+----------+----------+\n",
      "|col_1|sum(index)|sum(col_1)|sum(col_2)|sum(col_3)|sum(col_4)|sum(col_5)|sum(col_6)|\n",
      "+-----+----------+----------+----------+----------+----------+----------+----------+\n",
      "|   27|        41|        27|        36|        37|        41|        43|        45|\n",
      "|   26|      1789|        52|        56|        58|        75|        84|        87|\n",
      "|   12|     15480|       432|       646|       862|      1058|      1287|      1474|\n",
      "|   22|      2449|        88|       110|       123|       149|       158|       169|\n",
      "|    1|     75932|       145|      1105|      2318|      3357|      4384|      5623|\n",
      "+-----+----------+----------+----------+----------+----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby -- pandas 하고 비슷함 agg 쓰는 것도\n",
    "df_pyspark.groupby('col_1').sum().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840dd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
